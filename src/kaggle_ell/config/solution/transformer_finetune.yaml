name: transformer_finetune
args:
  train:
    apex: True
    print_freq: 20
    num_workers: 1
    scheduler: cosine
    batch_scheduler: True
    num_cycles: 0.5
    num_warmup_steps: 0
    epochs: 4
    encoder_lr: 2e-5
    decoder_lr: 2e-5
    min_lr: 1e-6
    eps: 1e-6
    betas: [0.9, 0.999]
    batch_size: 8
    weight_decay: 0.01
    gradient_accumulation_steps: 1
    max_grad_norm: 1000
    n_fold: 4
    trn_fold: [0, 1, 2, 3]
    seed: 42
  model:
    backbone: "google/bert_uncased_L-2_H-128_A-2"
    gradient_checkpointing: True
  inference:
    batch_size: 128
    num_workers: 1
  data:
    max_data_samples: 10
    max_length: 512



